---
title: "05 - Hypothesis Testing"
---

Mini Project 5: Hypothesis Testing

1.  *Towards the end of Section 1*, the authors say "As 'statistical significance' is used less, statistical thinking will be used more." Elaborate on what you think the authors mean. Give some examples of what you think embodies "statistical thinking."

I think when the author said this, they think research should look beyond statistical significance which allows for more variety of results that can be interpreted a variety of ways. I think this is especially important today in a world where lots of things are becoming automated or AI generated. It takes a person, though, to be able to come up with a model or idea that might not fit the standard or that turns out not to be 'statistically significant' but may still make sense. When I took econometrics, and this is true for most economic research, the p value was barely looked at. It was more about thinking if the variable made sense, thinking about if there might be correlation between variables or endogeneity. An example from econometrics would be that often times you're asked to create a model using different variables. You could try to throw every variable into the model and everything is significant. But often variables are highly correlated or provide the same information. So even though they pass this significance test, you can simplify the model to get a more accurate prediction. This couldn't be done without statistical thinking. Also, the author isn't saying don't completely ignore the p value. It varies person to person and situation to situation for the level they care about. So, leaving it up to the user or reader to determine if they think it is reasonable to use is important. Being able to think this way without solely relying on statistical significance will be essential if doing research especially with emerging technology which doesn't necessarily have this ability to think significantly.

2.  *Section 2, third paragraph*: The authors state "A label of statistical significance adds nothing to what is already conveyed by the value of *p*; in fact, this dichotomization of p-values makes matters worse." Elaborate on what you think the authors mean.

The author is saying that the p value speaks for itself and that a label is not necessarily.  Not having a label allows the researcher to make their own decision about whether they want to include the variable or whatever it might be. If the followed what was the standard, saying it's not significant above a certain p value limits the possibilities for research. Getting rid of the label of statistical significance allows for both the researcher and the consumer of the output to make their own decision about whether they think it makes sense or not.  

3.  *Section 2, end of first column*: The authors state "For the integrity of scientific publishing and research dissemination, therefore, whether a p-value passes any arbitrary threshold should not be considered at all when deciding which results to present or highlight." Do you agree or disagree? How should it be decided which results to present/highlight in scientific publishing?

I think at least mentioning the p value makes sense in scientific publishing although it should not be the main focus by any means. The authors say that only including finding that have a certain p value creates bias, but I believe not including or looking at p values would create even more bias. It would make it very easy to skew results or create results that aren't accurate at all. That is why I think finding that don't pass a p value should be included if the researcher wants but the p values should be as well for the readers use. For an average reader or someone skimming over the research, it would be very easy to create inaccurate results to sway people. I also think including the p value allows the reader to agree or disagree with the model. Like what we did in class, for certain p values we can have 'moderate evidence', 'strong evidence', or 'no evidence'.  Having p values included and giving this insight to the reader is important for the reader to make a decision about whether they like this model or not. 

4.  *Section 3, end of page 2*: The authors state "The statistical community has not yet converged on a simple paradigm for the use of statistical inference in scientific research -- and in fact it may never do so. A one-size-fits-all approach to statistical inference is an inappropriate expectation." Do you agree or disagree? Explain.

I do agree that a one-size-fits-all approach is not a good standard to have for statistical inference. I think that since this technique is used in so many different sectors with such varying importance, one single approach to replace statistical significance is not appropriate, much like only looking at p values isn't appropriate in most cases. I think it could be possible to make categories that research and findings fall into and have a standard paradigm for each category, but I'm not entirely sure what that would look like.

5.  *Section 3.2*: The authors note that they are envisioning "a sort of 'statistical thoughtfulness'." What do you think "statistical thoughtfulness" means? What are some ways to demonstrate "statistical thoughtfulness" in an analysis?

Statistical analysis means that you have a reason for everything you are doing. I think first off; to be able to have statistical thoughtfulness, you need to have a strong foundational understanding of statistics and the data you are working with.  In econometrics, the very first thing we did before running any regressions or anything else, we needed to understand the data fully. In an analysis, to demonstrate statistical thoughtfulness, you need to do everything with a purpose. For example, you're not adding variables into a model to just to add variables. You're doing your research with the goal of including everything you think needs to be incorporated while aiming for parsimony.  I think also writing your exploratory process alonside finished results is a way to demonstrate to the consumer of the findings that you did your analysis with statistical thoughtfulness. Finally, having a section in your findings of any assumptions you made or places where you could have done something differently or better goes along with the statistical thoughtfulness.  

6.  *Section 3.2.4*: A few of the authors of papers in this special issue argue that some of the terminology used in statistics, such as "significance" and "confidence" can be misleading, and they propose the use of "compatibility" instead. What do you think they believe the problem is? Do you agree or disagree (that there is a problem and that changing the name will help)?

I think the authors think the problem is that the words significance and confidence are too charged with it being either right(significant) or wrong(insignificant). Changing it to compatible is more neutral, not necessarily a right or wrong but more of a 'it could work' but it should really be thought about using your statistical thoughtfulness. I do think changing the name could help, but only to some degree. Hearing a '95% confidence interval' surely makes people feel very, if not overly, confident about the correctness of the test, whatever the test may be.  As someone who is hearing compatibility rather than significance or confidence, it does seem less loaded, meaning I don't feel as strong in favor. But I also think if statisticians and researchers were to be a change from significance/confidence to compatibility, I think they would be used interchangeably and the distinction between them wouldn't be much over time.

7.  Find a quote or point that really strikes you (i.e., made you think). What is the quote (and tell me where to find it), and why does it stand out to you?

Third paragraph of section 5.

Goodman observes that statisticians alone cannot address the problem, and that "any approach involving only statisticians will not succeed." He calls on statisticians to ally themselves "both with scientists in other fields and with broader based, multidisciplinary scientific reform movements. What statisticians\
can do within our own discipline is important, but to effectively disseminate or implement virtually any method or policy, we need partners."

This section stuck me because I was thinking about this from the moment I started reading. I was wondering to myself how this change would come about. In econometrics we barely looked at p values, but in STAT 213 we rely very heavily on p values. It got me thinking whether some institutions and even if departments at St.  Lawrence have talked about abandoning p values or not teaching them as heavily. This article was published six years ago, so I wonder if this change is still trying to take place or how much traction it's gotten since 2019.
